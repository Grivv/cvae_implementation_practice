{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlvlab/data303/blob/main/Image_Generation_by_NormalizingFlows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g13DOStGtOOp"
      },
      "source": [
        "### 0. Import required package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEREixiCi4O9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTAJrG_XtTDe"
      },
      "source": [
        "### 1. Define 2D Gaussian base distribution & target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEULi3cHkmQV"
      },
      "outputs": [],
      "source": [
        "from typing import *\n",
        "\n",
        "def zero_log_det_like_z(z):\n",
        "    return torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "\n",
        "# Base Distribution - underlying in our latent space\n",
        "class DiagGaussian(nn.Module):\n",
        "    # Multivariate Gaussian distribution with diagonal covariance matrix\n",
        "\n",
        "    def __init__(self,\n",
        "                 shape:Union[int, list],\n",
        "                 trainable:bool=True) -> None:\n",
        "        \n",
        "        super().__init__()\n",
        "        if isinstance(shape, int):\n",
        "            shape = (shape,)\n",
        "        if isinstance(shape, list):\n",
        "            shape = tuple(shape)\n",
        "        self.shape = shape\n",
        "        self.n_dim = len(shape)\n",
        "        self.d = np.prod(shape)\n",
        "        if trainable:\n",
        "            self.loc = nn.Parameter(torch.zeros(1, *self.shape))\n",
        "            self.log_scale = nn.Parameter(torch.zeros(1, *self.shape))\n",
        "        else:\n",
        "            self.register_buffer(\"loc\", torch.zeros(1, *self.shape))\n",
        "            self.register_buffer(\"log_scale\", torch.zeros(1, *self.shape))\n",
        "        self.temperature = None  # Temperature parameter for annealed sampling\n",
        "\n",
        "    def forward(self,\n",
        "                num_samples:int=1,\n",
        "                context=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \n",
        "        eps = torch.randn(\n",
        "            (num_samples,) + self.shape, dtype=self.loc.dtype, device=self.loc.device\n",
        "        )\n",
        "        if self.temperature is None:\n",
        "            log_scale = self.log_scale\n",
        "        else:\n",
        "            log_scale = self.log_scale + np.log(self.temperature)\n",
        "        z = self.loc + torch.exp(log_scale) * eps\n",
        "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
        "            log_scale + 0.5 * torch.pow(eps, 2), list(range(1, self.n_dim + 1))\n",
        "        )\n",
        "        return z, log_p\n",
        "\n",
        "    def log_prob(self,\n",
        "                 z:torch.Tensor,\n",
        "                 context=None) -> torch.Tensor:\n",
        "        \n",
        "        if self.temperature is None:\n",
        "            log_scale = self.log_scale\n",
        "        else:\n",
        "            log_scale = self.log_scale + np.log(self.temperature)\n",
        "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
        "            log_scale + 0.5 * torch.pow((z - self.loc) / torch.exp(log_scale), 2),\n",
        "            list(range(1, self.n_dim + 1)),\n",
        "        )\n",
        "        return log_p\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int=1,\n",
        "               **kwargs) -> torch.Tensor:\n",
        "        # Samples from base distribution\n",
        "\n",
        "        z, _ = self.forward(num_samples, **kwargs)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5PCCYMatdzN"
      },
      "outputs": [],
      "source": [
        "class CircularGaussianMixture(nn.Module):\n",
        "    # 8 gaussian distribution\n",
        "    # More modes -> higher capacity to approximate distribution\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_modes:int=8):\n",
        "        super(CircularGaussianMixture, self).__init__()\n",
        "        self.n_modes = n_modes\n",
        "        self.register_buffer(\n",
        "            \"scale\", torch.tensor(2 / 3 * np.sin(np.pi / self.n_modes)).float()\n",
        "        )\n",
        "\n",
        "    def log_prob(self,\n",
        "                 z:torch.Tensor) -> torch.Tensor:\n",
        "        d = torch.zeros((len(z), 0), dtype=z.dtype, device=z.device)\n",
        "        for i in range(self.n_modes):\n",
        "            d_ = (\n",
        "                (z[:, 0] - 2 * np.sin(2 * np.pi / self.n_modes * i)) ** 2\n",
        "                + (z[:, 1] - 2 * np.cos(2 * np.pi / self.n_modes * i)) ** 2\n",
        "            ) / (2 * self.scale**2)\n",
        "            d = torch.cat((d, d_[:, None]), 1)\n",
        "        log_p = -torch.log(\n",
        "            2 * np.pi * self.scale**2 * self.n_modes\n",
        "        ) + torch.logsumexp(-d, 1)\n",
        "        return log_p\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int=1) -> torch.Tensor:\n",
        "        eps = torch.randn(\n",
        "            (num_samples, 2), dtype=self.scale.dtype, device=self.scale.device\n",
        "        )\n",
        "        phi = (\n",
        "            2\n",
        "            * np.pi\n",
        "            / self.n_modes\n",
        "            * torch.randint(0, self.n_modes, (num_samples,), device=self.scale.device)\n",
        "        )\n",
        "        loc = torch.stack((2 * torch.sin(phi), 2 * torch.cos(phi)), 1).type(eps.dtype)\n",
        "        return eps * self.scale + loc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_8q3g71zR8b"
      },
      "source": [
        "### 2. Define RealNVP models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm79-23AlzYe"
      },
      "outputs": [],
      "source": [
        "class Flow(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z):\n",
        "        raise NotImplementedError(\"Forward pass has not been implemented.\")\n",
        "\n",
        "    def inverse(self, z):\n",
        "        raise NotImplementedError(\"This flow has no algebraic inverse.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfQ9zaULo2ZN"
      },
      "outputs": [],
      "source": [
        "# Flow layers to reshape the latent features\n",
        "\n",
        "class Split(Flow):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor) -> tuple[list[torch.Tensor], torch.Tensor]:\n",
        "        z1, z2 = z.chunk(2, dim=1)\n",
        "        log_det = 0\n",
        "        return [z1, z2], log_det\n",
        "\n",
        "    def inverse(self,\n",
        "                z:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        z1, z2 = z\n",
        "        z = torch.cat([z1, z2], 1)\n",
        "        log_det = 0\n",
        "        return z, log_det\n",
        "\n",
        "\n",
        "\"\"\"\n",
        " Merge - Same as Split but with forward and backward pass interchanged\n",
        "\"\"\"\n",
        "class Merge(Split):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        return super().inverse(z)\n",
        "\n",
        "    def inverse(self,\n",
        "                z:torch.Tensor) -> tuple[list[torch.Tensor], torch.Tensor]:\n",
        "        return super().forward(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pakdGTO3w5P7"
      },
      "outputs": [],
      "source": [
        "class Permute(Flow):\n",
        "    def __init__(self,\n",
        "                 num_channels:int):\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        perm = torch.randperm(self.num_channels)\n",
        "        inv_perm = torch.empty_like(perm).scatter_(\n",
        "            dim=0, index=perm, src=torch.arange(self.num_channels)\n",
        "        )\n",
        "        self.register_buffer(\"perm\", perm)\n",
        "        self.register_buffer(\"inv_perm\", inv_perm)\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor,\n",
        "                context:Union[torch.Tensor, None]=None):\n",
        "        z = z[:, self.perm, ...]\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        return z, log_det\n",
        "\n",
        "    def inverse(self, z, context=None):\n",
        "        z = z[:, self.inv_perm, ...]\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        return z, log_det"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSmhgKYcpBpy"
      },
      "outputs": [],
      "source": [
        "class AffineCoupling(Flow):\n",
        "    def __init__(self,\n",
        "                 param_map:nn.Module):\n",
        "        super().__init__()\n",
        "        self.add_module(\"param_map\", param_map)\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor) -> tuple[list[torch.Tensor], torch.Tensor]:\n",
        "        z1, z2 = z\n",
        "        param = self.param_map(z1)\n",
        "        shift = param[:, 0::2, ...]\n",
        "        scale_ = param[:, 1::2, ...]\n",
        "        z2 = z2 * torch.exp(scale_) + shift\n",
        "        log_det = torch.sum(scale_, dim=list(range(1, shift.dim())))\n",
        "\n",
        "        return [z1, z2], log_det\n",
        "\n",
        "    def inverse(self,\n",
        "                z:torch.Tensor) -> tuple[list[torch.Tensor], torch.Tensor]:\n",
        "        z1, z2 = z\n",
        "        param = self.param_map(z1)\n",
        "        shift = param[:, 0::2, ...]\n",
        "        scale_ = param[:, 1::2, ...]\n",
        "        z2 = (z2 - shift) * torch.exp(-scale_)\n",
        "        log_det = -torch.sum(scale_, dim=list(range(1, shift.dim())))\n",
        "\n",
        "        return [z1, z2], log_det\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNkUqCewlhOQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Affine Coupling layer including split and merge operation\n",
        "\"\"\"\n",
        "\n",
        "class AffineCouplingBlock(Flow):\n",
        "    def __init__(self,\n",
        "                 param_map:nn.Module):\n",
        "        super().__init__()\n",
        "        self.flows = nn.ModuleList([])\n",
        "        # Split layer\n",
        "        self.flows += [Split()]\n",
        "        # Affine coupling layer\n",
        "        self.flows += [AffineCoupling(param_map)]\n",
        "        # Merge layer\n",
        "        self.flows += [Merge()]\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        log_det_tot = torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_det_tot += log_det\n",
        "\n",
        "        return z, log_det_tot\n",
        "\n",
        "    def inverse(self,\n",
        "                z:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        log_det_tot = torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_det_tot += log_det\n",
        "\n",
        "        return z, log_det_tot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5zToifulrpb"
      },
      "outputs": [],
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 q0:torch.Tensor,\n",
        "                 flows:list[nn.Module],\n",
        "                 p:torch.Tensor=None):\n",
        "        super().__init__()\n",
        "        self.q0 = q0 # Base distribution\n",
        "        self.flows = nn.ModuleList(flows) # list of flows\n",
        "        self.p = p # Target distribution\n",
        "\n",
        "    def forward(self,\n",
        "                z:torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        for flow in self.flows:\n",
        "            z, _ = flow(z)\n",
        "        return z\n",
        "\n",
        "    def forward_and_log_det(self,\n",
        "                            z:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        for flow in self.flows:\n",
        "            z, log_d = flow(z)\n",
        "            log_det += log_d\n",
        "        return z, log_det\n",
        "\n",
        "    def inverse(self,\n",
        "                x:torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            x, _ = self.flows[i].inverse(x)\n",
        "        return x\n",
        "\n",
        "    def inverse_and_log_det(self,\n",
        "                            x:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        log_det = torch.zeros(len(x), device=x.device)\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            x, log_d = self.flows[i].inverse(x)\n",
        "            log_det += log_d\n",
        "        return x, log_det\n",
        "\n",
        "    def forward_kld(self,\n",
        "                    x:torch.Tensor) -> torch.Tensor:\n",
        "        # Estimates forward KL divergence\n",
        "\n",
        "        log_q = torch.zeros(len(x), device=x.device)\n",
        "        z = x\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_q += log_det\n",
        "        log_q += self.q0.log_prob(z)\n",
        "        return -torch.mean(log_q)\n",
        "\n",
        "    def reverse_kld(self,\n",
        "                    num_samples:int=1,\n",
        "                    beta:float=1.0,\n",
        "                    score_fn:bool=True) -> torch.Tensor:\n",
        "        # Estimates reverse KL divergence\n",
        "\n",
        "        z, log_q_ = self.q0(num_samples)\n",
        "        log_q = torch.zeros_like(log_q_)\n",
        "        log_q += log_q_\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_q -= log_det\n",
        "        if not score_fn:\n",
        "            z_ = z\n",
        "            log_q = torch.zeros(len(z_), device=z_.device)\n",
        "\n",
        "            for param in self.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "            for i in range(len(self.flows) - 1, -1, -1):\n",
        "                z_, log_det = self.flows[i].inverse(z_)\n",
        "                log_q += log_det\n",
        "            log_q += self.q0.log_prob(z_)\n",
        "\n",
        "            for param in self.parameters():\n",
        "              param.requires_grad = True\n",
        "\n",
        "        log_p = self.p.log_prob(z)\n",
        "        return torch.mean(log_q) - beta * torch.mean(log_p)\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int=1) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Samples from flow-based approximate distribution\n",
        "        \"\"\"\n",
        "        z, log_q = self.q0(num_samples)\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_q -= log_det\n",
        "        return z, log_q\n",
        "\n",
        "    def log_prob(self,\n",
        "                 x:torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        log_q = torch.zeros(len(x), dtype=x.dtype, device=x.device)\n",
        "        z = x\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_q += log_det\n",
        "        log_q += self.q0.log_prob(z)\n",
        "        return log_q\n",
        "\n",
        "    def save(self,\n",
        "             path:str) -> None:\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self,\n",
        "             path:str) -> None:\n",
        "        self.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VuF7JuOlECj"
      },
      "outputs": [],
      "source": [
        "class ConstScaleLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 scale:float=1.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.scale_cpu = torch.tensor(scale)\n",
        "        self.register_buffer(\"scale\", self.scale_cpu)\n",
        "\n",
        "    def forward(self,\n",
        "                input:torch.Tensor) -> torch.Tensor:\n",
        "        return input * self.scale\n",
        "\n",
        "class ClampExp(nn.Module):\n",
        "    \"\"\"\n",
        "    Nonlinearity min(exp(lam * x), 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClampExp, self).__init__()\n",
        "\n",
        "    def forward(self,\n",
        "                x:torch.Tensor) -> torch.Tensor:\n",
        "        one = torch.tensor(1.0, device=x.device, dtype=x.dtype)\n",
        "        return torch.min(torch.exp(x), one)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers:list[int],\n",
        "        leaky:float=0.0,\n",
        "        score_scale:Union[float, None]=None,\n",
        "        output_fn:Union[str, None]=None,\n",
        "        output_scale:Union[float, None]=None,\n",
        "        init_zeros:bool=False,\n",
        "        dropout:Union[float, None]=None,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        net = nn.ModuleList([])\n",
        "        for k in range(len(layers) - 2):\n",
        "            net.append(nn.Linear(layers[k], layers[k + 1]))\n",
        "            net.append(nn.LeakyReLU(leaky))\n",
        "        if dropout is not None:\n",
        "            net.append(nn.Dropout(p=dropout))\n",
        "        net.append(nn.Linear(layers[-2], layers[-1]))\n",
        "        if init_zeros:\n",
        "            nn.init.zeros_(net[-1].weight)\n",
        "            nn.init.zeros_(net[-1].bias)\n",
        "        if output_fn is not None:\n",
        "            if score_scale is not None:\n",
        "                net.append(ConstScaleLayer(score_scale))\n",
        "            if output_fn == \"sigmoid\":\n",
        "                net.append(nn.Sigmoid())\n",
        "            elif output_fn == \"relu\":\n",
        "                net.append(nn.ReLU())\n",
        "            elif output_fn == \"tanh\":\n",
        "                net.append(nn.Tanh())\n",
        "            elif output_fn == \"clampexp\":\n",
        "                net.append(ClampExp())\n",
        "            else:\n",
        "                NotImplementedError(\"This output function is not implemented.\")\n",
        "            if output_scale is not None:\n",
        "                net.append(ConstScaleLayer(output_scale))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self,\n",
        "                x:torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGURvVOf2mAC"
      },
      "source": [
        "### 5. Training for MNIST datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2se6HKmH4RWB"
      },
      "source": [
        "5-1. Prepares MNIST datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwm5cmSbpTGs",
        "outputId": "6a0379f5-7810-44bf-e0b1-11e2cfdef653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 116527120.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting datasets/MNIST/raw/train-images-idx3-ubyte.gz to datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27178751.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting datasets/MNIST/raw/train-labels-idx1-ubyte.gz to datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32054522.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18300219.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data\n",
        "batch_size = 128\n",
        "\n",
        "transform = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
        "                                   tv.transforms.Normalize((0.5), (1.0))])\n",
        "\n",
        "class_label = [1,2,3] # Or Set it as you want [1], [4], [1, 9], ... digit 0~9\n",
        "train_dataset = tv.datasets.MNIST('datasets/', train=True, download=True, transform=transform)\n",
        "class_one_indices = [i for i, (_, label) in enumerate(train_dataset) if label in class_label]\n",
        "class_one_dataset_train = torch.utils.data.Subset(train_dataset, class_one_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(class_one_dataset_train,  batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "test_dataset = tv.datasets.MNIST('datasets/', train=False, download=True, transform=transform)\n",
        "class_one_indices = [i for i, (_, label) in enumerate(test_dataset) if label == class_label]\n",
        "class_one_dataset_test = torch.utils.data.Subset(test_dataset, class_one_indices)\n",
        "test_loader = torch.utils.data.DataLoader(class_one_dataset_test, batch_size=batch_size)\n",
        "\n",
        "train_iter = iter(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTLlxrdm4BY8"
      },
      "source": [
        "** 5-2. Define your own NF model - Quiz\n",
        "* Write down answer1~4 and Train you model on MNIST datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7zwnaUB4A6k"
      },
      "outputs": [],
      "source": [
        "# Define Gaussian base distribution\n",
        "answer1 = 28 * 28 # Dimensionality of MNIST dataset which is 28 × 28 black-and-white images\n",
        "answer2 = answer1 // 2 # Dimensionality after split into half\n",
        "answer3 = 2 * answer2 # Dimensionality for scaling AND shift factors\n",
        "answer4 = answer1 # Dimensionality of the output of affine function -- for entire features\n",
        "\n",
        "mnist_base = DiagGaussian(answer1)\n",
        "\n",
        "# Define list of flows\n",
        "num_layers = 32 # Or Set it as you want (integer) 8, 12, 16, 32, ...\n",
        "flows = []\n",
        "for i in range(num_layers):\n",
        "    param_map = MLP([answer2, 64, 64, answer3], init_zeros=True)\n",
        "    # Add flow layer\n",
        "    flows.append(AffineCouplingBlock(param_map))\n",
        "    # Swap dimensions\n",
        "    flows.append(Permute(answer4))\n",
        "\n",
        "# Construct flow model\n",
        "my_model = NormalizingFlow(mnist_base, flows)\n",
        "my_model = my_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnXTMX6qDHyq"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "max_iter = 10000\n",
        "\n",
        "my_model.train()\n",
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "loss_hist = np.array([])\n",
        "\n",
        "for i in tqdm(range(max_iter)):\n",
        "    try:\n",
        "        x, y = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        x, y = next(train_iter)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = my_model.forward_kld(x.reshape(batch_size, -1).to(device))\n",
        "\n",
        "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_hist = np.append(loss_hist, loss.detach().to('cpu').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txjIAYpfDG-2"
      },
      "outputs": [],
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(loss_hist, label='loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dfyndeS0kXC"
      },
      "outputs": [],
      "source": [
        "# Model samples\n",
        "num_sample = 15\n",
        "z_sample, _ = my_model.sample(num_sample)\n",
        "z_sample = z_sample.reshape(num_sample, 28, 28).unsqueeze(1)\n",
        "z_sample = torch.clamp(z_sample, 0., 1.)\n",
        "plt.figure(figsize=(5, 15))\n",
        "plt.imshow(np.transpose(tv.utils.make_grid(z_sample, nrow=3).cpu().numpy(), (1, 2, 0)))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNgfKO0snhUumV9/OXIf3Y6",
      "collapsed_sections": [
        "g13DOStGtOOp"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
