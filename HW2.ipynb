{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc34be53-123a-4feb-80c6-7f4cc590dbb8",
      "metadata": {
        "id": "fc34be53-123a-4feb-80c6-7f4cc590dbb8"
      },
      "source": [
        "HW2. Conditional Variational Autoencoders (CVAE)\n",
        "======\n",
        "Implement a deep learning model that generates images of a given number, based on the CVAE. (Take a look at the Lab1 PowerPoint)\n",
        "\n",
        "There are **three code blocks** to be implemented.\n",
        "[Encoder, Decoder, Conditional Variational Autoencoder]\n",
        "\n",
        "\n",
        "\n",
        "Submit a PDF file (a 1~2 pages report) and your code.\n",
        "Your report should contain the following.\n",
        "\n",
        "- How the model architecture was modified to incorporate class condition. (Just take a screenshot of your code.)\n",
        "- Discuss the comparison of reconstruction errors between VAE and CVAE.\n",
        "- You may observe that images not corresponding to the conditions are also generated. Discuss the reasons and propose methods to resolve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca30703c-522a-4f6d-bb3a-ebb5cef60140",
      "metadata": {
        "id": "ca30703c-522a-4f6d-bb3a-ebb5cef60140"
      },
      "outputs": [],
      "source": [
        "# This is for colab users to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb417cef-1919-4715-a314-5381345b9e41",
      "metadata": {
        "id": "cb417cef-1919-4715-a314-5381345b9e41"
      },
      "outputs": [],
      "source": [
        "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
        "import sys\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a536ef6-a206-4e00-9693-628a48bf1906",
      "metadata": {
        "id": "1a536ef6-a206-4e00-9693-628a48bf1906",
        "tags": []
      },
      "source": [
        "Hyperparameters\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013b7902-58a4-417e-a965-b97f3313f46d",
      "metadata": {
        "id": "013b7902-58a4-417e-a965-b97f3313f46d"
      },
      "outputs": [],
      "source": [
        "# 2-d latent space, parameter count in the same order of magnitude\n",
        "# as in the original VAE paper (VAE paper has about 3x as many)\n",
        "latent_dims = 2\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "variational_beta = 1\n",
        "use_gpu = True\n",
        "savepath='cvae_2d_100.pth'\n",
        "\n",
        "# # 10-d latent space, for comparison with non-variational auto-encoder\n",
        "# latent_dims = 10\n",
        "# num_epochs = 100\n",
        "# batch_size = 128\n",
        "# capacity = 64\n",
        "# learning_rate = 1e-3\n",
        "# variational_beta = 1\n",
        "# use_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146e2cbf-3ee6-4242-bac2-fdfe43b48060",
      "metadata": {
        "id": "146e2cbf-3ee6-4242-bac2-fdfe43b48060"
      },
      "source": [
        "MNIST Data Loading\n",
        "-------------------\n",
        "\n",
        "MNIST images show digits from 0-9 in 28x28 grayscale images. We do not center them at 0, because we will be using a binary cross-entropy loss that treats pixel values as probabilities in [0,1]. We create both a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2b8892-a1bb-4886-8c60-316b2b66152e",
      "metadata": {
        "id": "eb2b8892-a1bb-4886-8c60-316b2b66152e"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PTQCBmnv8SNc",
      "metadata": {
        "id": "PTQCBmnv8SNc"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "idx = 42\n",
        "# train_dataset[idx] := (image data, class)\n",
        "print(train_dataset[idx][1]) # class\n",
        "to_pil_image(train_dataset[idx][0]) # image data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee61fbf-2725-4d86-a295-d3a7cd4091cb",
      "metadata": {
        "id": "4ee61fbf-2725-4d86-a295-d3a7cd4091cb"
      },
      "source": [
        "CVAE Definition*\n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fk1-SxbcL5M5",
      "metadata": {
        "id": "Fk1-SxbcL5M5"
      },
      "source": [
        "### Encoder*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nIulgHU0L8bI",
      "metadata": {
        "id": "nIulgHU0L8bI"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "\n",
        "        # Result of concatenation: additional channel indicating class information\n",
        "        # i.e. input channel consists of 1 channel for input image and 1 channel for class information\n",
        "        # 2 channels in total for the first conv layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "        # Log to variance makes the model more stable\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "\n",
        "    def forward(self, x, condition): # modified\n",
        "        # Simple way to add class information: concatenate to first conv layer\n",
        "        condition = (condition.unsqueeze(1).unsqueeze(2).unsqueeze(3)*torch.ones((x.size(0),1,x.size(2),x.size(3)), device=x.device)) ## condition.shape = [b,1,w,h]\n",
        "        x = torch.concat((x, condition), 1) \n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uCga32afL_CP",
      "metadata": {
        "id": "uCga32afL_CP"
      },
      "source": [
        "### Decoder*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QF2sAQT2MAiU",
      "metadata": {
        "id": "QF2sAQT2MAiU"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        # Basically the reverse order of encoder\n",
        "        # Plus additional dimension for class information\n",
        "        self.fc = nn.Linear(in_features=latent_dims+1, out_features=c*2*7*7)\n",
        "\n",
        "        # Transpose conv layer for upscaling (deconvolution)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, condition): #modified\n",
        "        # Simple way to add class information: concatenate to first deconv layer\n",
        "        condition = condition.unsqueeze(1) ## condition.shape = [b,1]\n",
        "        x = torch.concat((x, condition), 1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.sigmoid(self.conv1(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QvbCV5TZMDdM",
      "metadata": {
        "id": "QvbCV5TZMDdM"
      },
      "source": [
        "### Conditional Variational Autoencoder*\n",
        "\n",
        "> 들여쓴 블록\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be834102-848b-47d6-afd6-36a825101c15",
      "metadata": {
        "id": "be834102-848b-47d6-afd6-36a825101c15"
      },
      "outputs": [],
      "source": [
        "class ConditionalVariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConditionalVariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x, condition):\n",
        "\n",
        "        # Add condition tensor for running forward\n",
        "        latent_mu, latent_logvar = self.encoder(x, condition)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent, condition)\n",
        "\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            '''\n",
        "            logvar = log(s^2)\n",
        "            0.5 * logvar = 0.5 * log(s^2) = log(s^(2 * 0.5)) = log(s)\n",
        "            .exp_() means 'to exponentiate'\n",
        "            exp(log(s)) = e^(log(s)) = s\n",
        "            '''\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_() # normal distribution\n",
        "            return eps.mul(std).add_(mu) # e * s + m\n",
        "        else:\n",
        "            return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A6zquYoBMMbK",
      "metadata": {
        "id": "A6zquYoBMMbK"
      },
      "outputs": [],
      "source": [
        "cvae = ConditionalVariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "cvae = cvae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in cvae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b681401-e9d0-44ef-b07f-5ecbe3e66c4c",
      "metadata": {
        "id": "5b681401-e9d0-44ef-b07f-5ecbe3e66c4c"
      },
      "source": [
        "Train CVAE\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mF9eecwrMWdA",
      "metadata": {
        "id": "mF9eecwrMWdA"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUAlD6o2LjID",
      "metadata": {
        "id": "MUAlD6o2LjID"
      },
      "outputs": [],
      "source": [
        "def cvae_loss(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return recon_loss + variational_beta * kldivergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w2_vqywuMZlv",
      "metadata": {
        "id": "w2_vqywuMZlv"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f5d63b-d1ce-4194-b54f-a0716a500ee0",
      "metadata": {
        "id": "39f5d63b-d1ce-4194-b54f-a0716a500ee0"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=cvae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "cvae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    for image_batch, condition_batch in train_dataloader: # modified - class label is needed in C-VAE\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "        condition_batch = condition_batch.to(device) #modified\n",
        "\n",
        "        image_batch_recon, latent_mu, latent_logvar = cvae(image_batch,condition_batch) #modified\n",
        "\n",
        "        loss = cvae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s875aDOtMzG0",
      "metadata": {
        "id": "s875aDOtMzG0"
      },
      "outputs": [],
      "source": [
        "torch.save(cvae.state_dict(), 'gdrive/My Drive/Colab Notebooks/'+savepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420168c7-bcdd-40a3-b1a7-550f00210d56",
      "metadata": {
        "id": "420168c7-bcdd-40a3-b1a7-550f00210d56"
      },
      "source": [
        "### Plot Training Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e49925-f0d9-4e1f-882a-ae179e73615c",
      "metadata": {
        "id": "29e49925-f0d9-4e1f-882a-ae179e73615c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7395b6b2-30d8-4c5b-a954-e51693e6eb3b",
      "metadata": {
        "id": "7395b6b2-30d8-4c5b-a954-e51693e6eb3b"
      },
      "source": [
        "Experiments\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XK834ZlgMhHv",
      "metadata": {
        "id": "XK834ZlgMhHv"
      },
      "source": [
        "### Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71356d89-ecee-4efb-bb36-facebd42e3f4",
      "metadata": {
        "id": "71356d89-ecee-4efb-bb36-facebd42e3f4"
      },
      "outputs": [],
      "source": [
        "cvae.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, condition_batch in test_dataloader: #modified\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "        condition_batch = condition_batch.to(device) #modified\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = cvae(image_batch,condition_batch) #modified\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = cvae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c406e791-8f6e-4504-a657-15771b4ff34a",
      "metadata": {
        "id": "c406e791-8f6e-4504-a657-15771b4ff34a"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52028e82-0576-4608-9f17-0ed57f4bab51",
      "metadata": {
        "id": "52028e82-0576-4608-9f17-0ed57f4bab51"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "cvae.eval()\n",
        "\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, conditions, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        conditions = conditions.to(device)\n",
        "        images, _, _ = model(images, conditions)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = next(iter(test_dataloader)) # Bug Fix\n",
        "\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(images, labels, cvae)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cdf28b3-2aea-4401-8b08-3c6bf83e1fac",
      "metadata": {
        "id": "7cdf28b3-2aea-4401-8b08-3c6bf83e1fac"
      },
      "source": [
        "### Conditional Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b0fd88-e8ba-4188-9725-35b3b6577505",
      "metadata": {
        "id": "f9b0fd88-e8ba-4188-9725-35b3b6577505"
      },
      "outputs": [],
      "source": [
        "cvae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Sampling from normal distribution - prior probability\n",
        "    latent = torch.randn(100, latent_dims, device=device)\n",
        "    condition = torch.tensor([i//10 for i in range(100)], device=device) #modified\n",
        "\n",
        "    img_recon = cvae.decoder(latent, condition) #modified\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
